{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GEgzVdSfaPI",
        "colab_type": "code",
        "outputId": "a5b98f78-8d1a-4327-8d05-e2ba31546c74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# 加载 google 云盘"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EkHR2w-f76-",
        "colab_type": "code",
        "outputId": "9ed1d2bb-b99a-4d8d-ddf9-cc56898fa16d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!cat /usr/local/cuda/version.txt\n",
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()\n",
        "# CUDA Version 10.1.243"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA Version 10.1.243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-PEo9yu0Xua",
        "colab_type": "code",
        "outputId": "46b9501e-24ee-458e-e784-2520f6377816",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm   # 进度条\n",
        "import warnings\n",
        "\n",
        "# path = '/content/drive/My Drive/modelnet40_normal_resampled/modelnet40_shape_names.txt'\n",
        "# open(path)\n",
        "print(torch.cuda.is_available())\n",
        "torch.backends.cudnn.enabled = False  \n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwdJNAtpBRsQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ModelNet40 数据集加载\n",
        "def pc_normalize(pc):\n",
        "    centroid = np.mean(pc, axis=0)\n",
        "    pc = pc - centroid\n",
        "    m = np.max(np.sqrt(np.sum(pc**2, axis=1)))\n",
        "    pc = pc / m\n",
        "    return pc\n",
        "\n",
        "class ModelNetDataLoader(Dataset):\n",
        "  def __init__(self, root,  npoint=1024, split='train', uniform=False, normal_channel=True, cache_size=15000):\n",
        "    self.root = root\n",
        "    self.npoints = npoint\n",
        "    self.uniform = uniform\n",
        "    self.catfile = os.path.join(self.root, 'modelnet40_shape_names.txt')\n",
        "\n",
        "    self.cat = [line.rstrip() for line in open(self.catfile)]\n",
        "    self.classes = dict(zip(self.cat, range(len(self.cat))))\n",
        "    self.normal_channel = normal_channel\n",
        "\n",
        "    shape_ids = {}\n",
        "    shape_ids['train'] = [line.rstrip() for line in open(os.path.join(self.root, 'modelnet40_train.txt'))]\n",
        "    shape_ids['test'] = [line.rstrip() for line in open(os.path.join(self.root, 'modelnet40_test.txt'))]\n",
        "\n",
        "    assert (split == 'train' or split == 'test')\n",
        "    shape_names = ['_'.join(x.split('_')[0:-1]) for x in shape_ids[split]]\n",
        "    # list of (shape_name, shape_txt_file_path) tuple\n",
        "    self.datapath = [(shape_names[i], os.path.join(self.root, shape_names[i], shape_ids[split][i]) + '.txt') for i\n",
        "                      in range(len(shape_ids[split]))]\n",
        "    print('The size of %s data is %d'%(split,len(self.datapath)))\n",
        "\n",
        "    self.cache_size = cache_size  # how many data points to cache in memory\n",
        "    self.cache = {}  # from index to (point_set, cls) tuple\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.datapath)\n",
        "\n",
        "  def _get_item(self, index):\n",
        "    if index in self.cache:\n",
        "      point_set, cls = self.cache[index]\n",
        "    else:\n",
        "      fn = self.datapath[index]\n",
        "      cls = self.classes[self.datapath[index][0]]\n",
        "      cls = np.array([cls]).astype(np.int32)\n",
        "      point_set = np.loadtxt(fn[1], delimiter=',').astype(np.float32)\n",
        "      if self.uniform:\n",
        "        point_set = self.farthest_point_sample(point_set, self.npoints)\n",
        "      else:\n",
        "        point_set = point_set[0:self.npoints,:]\n",
        "\n",
        "      point_set[:, 0:3] = pc_normalize(point_set[:, 0:3])\n",
        "\n",
        "      if not self.normal_channel:\n",
        "        point_set = point_set[:, 0:3]\n",
        "\n",
        "      if len(self.cache) < self.cache_size:\n",
        "        self.cache[index] = (point_set, cls)\n",
        "\n",
        "    return point_set, cls\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self._get_item(index)\n",
        "\n",
        "  def farthest_point_sample(point, npoint):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        xyz: pointcloud data, [N, D]\n",
        "        npoint: number of samples\n",
        "    Return:\n",
        "        centroids: sampled pointcloud index, [npoint, D]\n",
        "    \"\"\"\n",
        "    N, D = point.shape\n",
        "    xyz = point[:,:3]\n",
        "    centroids = np.zeros((npoint,))\n",
        "    distance = np.ones((N,)) * 1e10\n",
        "    farthest = np.random.randint(0, N)\n",
        "    for i in range(npoint):\n",
        "        centroids[i] = farthest\n",
        "        centroid = xyz[farthest, :]\n",
        "        dist = np.sum((xyz - centroid) ** 2, -1)\n",
        "        mask = dist < distance\n",
        "        distance[mask] = dist[mask]\n",
        "        farthest = np.argmax(distance, -1)\n",
        "    point = point[centroids.astype(np.int32)]\n",
        "    return point\n",
        "\n",
        "# test\n",
        "# data = ModelNetDataLoader('/content/drive/My Drive/modelnet40_normal_resampled/',split='train', uniform=False, normal_channel=True,)\n",
        "# DataLoader = torch.utils.data.DataLoader(data, batch_size=12, shuffle=True)\n",
        "# for point,label in DataLoader:\n",
        "#   print(point.shape)\n",
        "#   print(label.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQXrOqm3BU0H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def index_points(points, idx):\n",
        "  \"\"\"\n",
        "\n",
        "  Input:\n",
        "      points: input points data, [B, N, C]\n",
        "      idx: sample index data, [B, S]\n",
        "  Return:\n",
        "      new_points:, indexed points data, [B, S, C]\n",
        "  \"\"\"\n",
        "  device = points.device\n",
        "  B = points.shape[0]\n",
        "  view_shape = list(idx.shape)\n",
        "  view_shape[1:] = [1] * (len(view_shape) - 1)\n",
        "  repeat_shape = list(idx.shape)\n",
        "  repeat_shape[0] = 1\n",
        "  batch_indices = torch.arange(B, dtype=torch.long).to(device).view(view_shape).repeat(repeat_shape)\n",
        "  new_points = points[batch_indices, idx, :]\n",
        "  return new_points\n",
        "\n",
        "def square_distance(src, dst):\n",
        "  \"\"\"\n",
        "  Calculate Euclid distance between each two points.\n",
        "\n",
        "  src^T * dst = xn * xm + yn * ym + zn * zm；\n",
        "  sum(src^2, dim=-1) = xn*xn + yn*yn + zn*zn;\n",
        "  sum(dst^2, dim=-1) = xm*xm + ym*ym + zm*zm;\n",
        "  dist = (xn - xm)^2 + (yn - ym)^2 + (zn - zm)^2\n",
        "        = sum(src**2,dim=-1)+sum(dst**2,dim=-1)-2*src^T*dst\n",
        "\n",
        "  Input:\n",
        "      src: source points, [B, N, C]\n",
        "      dst: target points, [B, M, C]\n",
        "  Output:\n",
        "      dist: per-point square distance, [B, N, M]\n",
        "  \"\"\"\n",
        "  B, N, _ = src.shape\n",
        "  _, M, _ = dst.shape\n",
        "  dist = -2 * torch.matmul(src, dst.permute(0, 2, 1))\n",
        "  dist += torch.sum(src ** 2, -1).view(B, N, 1)\n",
        "  dist += torch.sum(dst ** 2, -1).view(B, 1, M)\n",
        "  return dist\n",
        "\n",
        "\n",
        "def query_ball_point(radius, nsample, xyz, new_xyz):\n",
        "  \"\"\"\n",
        "  Input:\n",
        "      radius: local region radius\n",
        "      nsample: max sample number in local region\n",
        "      xyz: all points, [B, N, 3]\n",
        "      new_xyz: query points, [B, S, 3]\n",
        "  Return:\n",
        "      group_idx: grouped points index, [B, S, nsample]\n",
        "  \"\"\"\n",
        "  device = xyz.device\n",
        "  B, N, C = xyz.shape\n",
        "  _, S, _ = new_xyz.shape\n",
        "  group_idx = torch.arange(N, dtype=torch.long).to(device).view(1, 1, N).repeat([B, S, 1])\n",
        "  sqrdists = square_distance(new_xyz, xyz)\n",
        "  group_idx[sqrdists > radius ** 2] = N\n",
        "  group_idx = group_idx.sort(dim=-1)[0][:, :, :nsample]\n",
        "  group_first = group_idx[:, :, 0].view(B, S, 1).repeat([1, 1, nsample])\n",
        "  mask = group_idx == N\n",
        "  group_idx[mask] = group_first[mask]\n",
        "  return group_idx\n",
        "\n",
        "\n",
        "def farthest_point_sample(xyz, npoint):\n",
        "  \"\"\"\n",
        "  Input:\n",
        "      xyz: pointcloud data, [B, N, 3]\n",
        "      npoint: number of samples\n",
        "  Return:\n",
        "      centroids: sampled pointcloud index, [B, npoint]\n",
        "  \"\"\"\n",
        "  device = xyz.device\n",
        "  B, N, C = xyz.shape\n",
        "  centroids = torch.zeros(B, npoint, dtype=torch.long).to(device)\n",
        "  distance = torch.ones(B, N).to(device) * 1e10\n",
        "  farthest = torch.randint(0, N, (B,), dtype=torch.long).to(device)\n",
        "  batch_indices = torch.arange(B, dtype=torch.long).to(device)\n",
        "  # print(B,N,C)\n",
        "  for i in range(npoint):\n",
        "    centroids[:, i] = farthest\n",
        "    centroid_temp = xyz[batch_indices, farthest, :].view(B, 1, 3)\n",
        "    dist = torch.sum((xyz - centroid_temp) ** 2, -1)\n",
        "    mask = dist < distance\n",
        "    distance[mask] = dist[mask]\n",
        "    farthest = torch.max(distance, -1)[1]\n",
        "  return centroids\n",
        "\n",
        "def sample_and_group(npoint, radius, nsample, xyz, points, returnfps=False):\n",
        "  \"\"\"\n",
        "  Input:\n",
        "      npoint:\n",
        "      radius:\n",
        "      nsample:\n",
        "      xyz: input points position data, [B, N, 3]\n",
        "      points: input points data, [B, N, D]\n",
        "  Return:\n",
        "      new_xyz: sampled points position data, [B, npoint, nsample, 3]\n",
        "      new_points: sampled points data, [B, npoint, nsample, 3+D]\n",
        "  \"\"\"\n",
        "  B, N, C = xyz.shape\n",
        "  S = npoint\n",
        "  fps_idx = farthest_point_sample(xyz, npoint) # [B, npoint, C]\n",
        "  torch.cuda.empty_cache()\n",
        "  new_xyz = index_points(xyz, fps_idx)\n",
        "  torch.cuda.empty_cache()\n",
        "  idx = query_ball_point(radius, nsample, xyz, new_xyz)\n",
        "  torch.cuda.empty_cache()\n",
        "  grouped_xyz = index_points(xyz, idx) # [B, npoint, nsample, C]\n",
        "  torch.cuda.empty_cache()\n",
        "  grouped_xyz_norm = grouped_xyz - new_xyz.view(B, S, 1, C)\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  if points is not None:\n",
        "      grouped_points = index_points(points, idx)\n",
        "      new_points = torch.cat([grouped_xyz_norm, grouped_points], dim=-1) # [B, npoint, nsample, C+D]\n",
        "  else:\n",
        "      new_points = grouped_xyz_norm\n",
        "  if returnfps:\n",
        "      return new_xyz, new_points, grouped_xyz, fps_idx\n",
        "  else:\n",
        "      return new_xyz, new_points\n",
        "\n",
        "\n",
        "def sample_and_group_all(xyz, points):\n",
        "  \"\"\"\n",
        "  Input:\n",
        "      xyz: input points position data, [B, N, 3]\n",
        "      points: input points data, [B, N, D]\n",
        "  Return:\n",
        "      new_xyz: sampled points position data, [B, 1, 3]\n",
        "      new_points: sampled points data, [B, 1, N, 3+D]\n",
        "  \"\"\"\n",
        "  device = xyz.device\n",
        "  B, N, C = xyz.shape\n",
        "  new_xyz = torch.zeros(B, 1, C).to(device)\n",
        "  grouped_xyz = xyz.view(B, 1, N, C)\n",
        "  if points is not None:\n",
        "      new_points = torch.cat([grouped_xyz, points.view(B, 1, N, -1)], dim=-1)\n",
        "  else:\n",
        "      new_points = grouped_xyz\n",
        "  return new_xyz, new_points\n",
        "\n",
        "# SA层\n",
        "class PointNetSetAbstraction(nn.Module):\n",
        "  def __init__(self, npoint, radius, nsample, in_channel, mlp, group_all):\n",
        "    super(PointNetSetAbstraction, self).__init__()\n",
        "    self.npoint = npoint\n",
        "    self.radius = radius\n",
        "    self.nsample = nsample\n",
        "    self.mlp_convs = nn.ModuleList()\n",
        "    self.mlp_bns = nn.ModuleList()\n",
        "    last_channel = in_channel\n",
        "    for out_channel in mlp:\n",
        "        self.mlp_convs.append(nn.Conv2d(last_channel, out_channel, 1))\n",
        "        self.mlp_bns.append(nn.BatchNorm2d(out_channel))\n",
        "        last_channel = out_channel\n",
        "    self.group_all = group_all\n",
        "\n",
        "  def forward(self, xyz, points):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        xyz: input points position data, [B, C, N]\n",
        "        points: input points data, [B, D, N]\n",
        "    Return:\n",
        "        new_xyz: sampled points position data, [B, C, S]\n",
        "        new_points_concat: sample points feature data, [B, D', S]\n",
        "    \"\"\"\n",
        "    xyz = xyz.permute(0, 2, 1)\n",
        "    if points is not None:\n",
        "        points = points.permute(0, 2, 1)\n",
        "\n",
        "    if self.group_all:\n",
        "        new_xyz, new_points = sample_and_group_all(xyz, points)\n",
        "    else:\n",
        "        new_xyz, new_points = sample_and_group(self.npoint, self.radius, self.nsample, xyz, points)\n",
        "    # new_xyz: sampled points position data, [B, npoint, C]\n",
        "    # new_points: sampled points data, [B, npoint, nsample, C+D]\n",
        "    new_points = new_points.permute(0, 3, 2, 1) # [B, C+D, nsample,npoint]\n",
        "    for i, conv in enumerate(self.mlp_convs):\n",
        "        bn = self.mlp_bns[i]\n",
        "        new_points =  F.relu(bn(conv(new_points)))\n",
        "\n",
        "    new_points = torch.max(new_points, 2)[0]\n",
        "    new_xyz = new_xyz.permute(0, 2, 1)\n",
        "    return new_xyz, new_points\n",
        "\n",
        "# PointNet++\n",
        "class Model(nn.Module):\n",
        "  def __init__(self,num_class,normal_channel=True):\n",
        "    super(Model, self).__init__()\n",
        "    in_channel = 6 if normal_channel else 3\n",
        "    self.normal_channel = normal_channel\n",
        "    self.sa1 = PointNetSetAbstraction(npoint=512, radius=0.2, nsample=32, in_channel=in_channel, mlp=[64, 64, 128], group_all=False)\n",
        "    self.sa2 = PointNetSetAbstraction(npoint=128, radius=0.4, nsample=64, in_channel=128 + 3, mlp=[128, 128, 256], group_all=False)\n",
        "    self.sa3 = PointNetSetAbstraction(npoint=None, radius=None, nsample=None, in_channel=256 + 3, mlp=[256, 512, 1024], group_all=True)\n",
        "    self.fc1 = nn.Linear(1024, 512)\n",
        "    self.bn1 = nn.BatchNorm1d(512)\n",
        "    self.drop1 = nn.Dropout(0.4)\n",
        "    self.fc2 = nn.Linear(512, 256)\n",
        "    self.bn2 = nn.BatchNorm1d(256)\n",
        "    self.drop2 = nn.Dropout(0.4)\n",
        "    self.fc3 = nn.Linear(256, num_class)\n",
        "\n",
        "  def forward(self, xyz):\n",
        "    B, _, _ = xyz.shape\n",
        "    if self.normal_channel:\n",
        "      norm = xyz[:, 3:, :]\n",
        "      xyz = xyz[:, :3, :]\n",
        "    else:\n",
        "      norm = None\n",
        "    l1_xyz, l1_points = self.sa1(xyz, norm)\n",
        "    l2_xyz, l2_points = self.sa2(l1_xyz, l1_points)\n",
        "    l3_xyz, l3_points = self.sa3(l2_xyz, l2_points)\n",
        "    x = l3_points.view(B, 1024)\n",
        "    x = self.drop1(F.relu(self.bn1(self.fc1(x))))\n",
        "    x = self.drop2(F.relu(self.bn2(self.fc2(x))))\n",
        "    x = self.fc3(x)\n",
        "    x = F.log_softmax(x, -1)\n",
        "    return x, l3_points\n",
        "\n",
        "class Loss(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Loss, self).__init__()\n",
        "\n",
        "  def forward(self, pred, target, trans_feat):\n",
        "    return F.nll_loss(pred, target)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orwW9DtKBXfI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def shift_point_cloud(batch_data, shift_range=0.1):\n",
        "  \"\"\" Randomly shift point cloud. Shift is per point cloud.\n",
        "      Input:\n",
        "        BxNx3 array, original batch of point clouds\n",
        "      Return:\n",
        "        BxNx3 array, shifted batch of point clouds\n",
        "  \"\"\"\n",
        "  B, N, C = batch_data.shape\n",
        "  shifts = np.random.uniform(-shift_range, shift_range, (B,3))\n",
        "  for batch_index in range(B):\n",
        "    batch_data[batch_index,:,:] += shifts[batch_index,:]\n",
        "  return batch_data\n",
        "\n",
        "\n",
        "def random_scale_point_cloud(batch_data, scale_low=0.8, scale_high=1.25):\n",
        "  \"\"\" Randomly scale the point cloud. Scale is per point cloud.\n",
        "      Input:\n",
        "          BxNx3 array, original batch of point clouds\n",
        "      Return:\n",
        "          BxNx3 array, scaled batch of point clouds\n",
        "  \"\"\"\n",
        "  B, N, C = batch_data.shape\n",
        "  scales = np.random.uniform(scale_low, scale_high, B)\n",
        "  for batch_index in range(B):\n",
        "    batch_data[batch_index,:,:] *= scales[batch_index]\n",
        "  return batch_data\n",
        "\n",
        "def random_point_dropout(batch_pc, max_dropout_ratio=0.875):\n",
        "  ''' batch_pc: BxNx3 '''\n",
        "  for b in range(batch_pc.shape[0]):\n",
        "    dropout_ratio =  np.random.random()*max_dropout_ratio # 0~0.875\n",
        "    drop_idx = np.where(np.random.random((batch_pc.shape[1]))<=dropout_ratio)[0]\n",
        "    if len(drop_idx)>0:\n",
        "        batch_pc[b,drop_idx,:] = batch_pc[b,0,:] # set to the first point\n",
        "  return batch_pc\n",
        "\n",
        "\n",
        "# test function\n",
        "def test(model, loader, vote_num = 0, num_class = 40):\n",
        "  mean_correct = []\n",
        "  class_acc = np.zeros((num_class,3))\n",
        "  for j, data in tqdm(enumerate(loader), total=len(loader)):\n",
        "    points, target = data\n",
        "    target = target[:, 0]\n",
        "    points = points.transpose(2, 1)\n",
        "    points, target = points.cuda(), target.cuda()\n",
        "    classifier = model.eval()\n",
        "    if vote_num > 0:\n",
        "      vote_pool = torch.zeros(target.size()[0],num_class).cuda()\n",
        "      # vote_pool = torch.zeros(target.size()[0],num_class)\n",
        "      for _ in range(vote_num):\n",
        "        pred, _ = classifier(points)\n",
        "        vote_pool += pred\n",
        "      pred = vote_pool/vote_num\n",
        "    else:\n",
        "      pred, _ = classifier(points)\n",
        "    pred_choice = pred.data.max(1)[1]\n",
        "    for cat in np.unique(target.cpu()):\n",
        "        classacc = pred_choice[target==cat].eq(target[target==cat].long().data).cpu().sum()\n",
        "        class_acc[cat,0]+= classacc.item()/float(points[target==cat].size()[0])\n",
        "        class_acc[cat,1]+=1\n",
        "    correct = pred_choice.eq(target.long().data).cpu().sum()\n",
        "    mean_correct.append(correct.item()/float(points.size()[0]))\n",
        "  class_acc[:,2] =  class_acc[:,0]/ class_acc[:,1]\n",
        "  class_acc = np.mean(class_acc[:,2])\n",
        "  instance_acc = np.mean(mean_correct)\n",
        "  return instance_acc, class_acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56qmUq9kQC_D",
        "colab_type": "code",
        "outputId": "c39335ca-1473-4450-dc8c-d7d387e29600",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        }
      },
      "source": [
        "# train\n",
        "'''parameters'''\n",
        "batch_size = 24\n",
        "train_epoch = 200\n",
        "learning_rate = 0.001\n",
        "decay_rate = 1e-4\n",
        "num_point = 1024\n",
        "num_class = 40\n",
        "data_path = '/content/drive/My Drive/modelnet40_normal_resampled/'\n",
        "model_save_path = '/content/drive/My Drive/best_model.pth'\n",
        "gpu = '0'\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu\n",
        "\n",
        "'''data loading'''\n",
        "print('load dataset....')\n",
        "train_dataset = ModelNetDataLoader(data_path,num_point,'train')\n",
        "train_data_loader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size,num_workers=4,shuffle=True)\n",
        "test_dataset = ModelNetDataLoader(data_path,num_point,'test')\n",
        "test_data_loader = torch.utils.data.DataLoader(test_dataset,batch_size=batch_size,num_workers=4,shuffle=False)\n",
        "print('data loaded\\n')\n",
        "\n",
        "'''model loading'''\n",
        "print('model loading')\n",
        "classifier = Model(num_class).cuda()\n",
        "criterion = Loss().cuda()\n",
        "\n",
        "try:\n",
        "  checkpoint = torch.load(model_save_path)\n",
        "  start_epoch = checkpoint['epoch']\n",
        "  classifier.load_state_dict(checkpoint['model_state_dict'])\n",
        "  print('Use pretrain model, current epoch: %d' % (start_epoch))\n",
        "except:\n",
        "  print('No existing model, starting training from scratch...')\n",
        "  start_epoch = 0\n",
        "\n",
        "optimizer = torch.optim.Adam(classifier.parameters(),lr=learning_rate,betas=(0.9, 0.999),eps=1e-08,weight_decay=decay_rate)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.7)\n",
        "\n",
        "global_epoch = 0\n",
        "best_instance_acc = 0.0\n",
        "best_class_acc = 0.0\n",
        "mean_correct = []\n",
        "print('model loaded\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "load dataset....\n",
            "The size of train data is 9843\n",
            "The size of test data is 2468\n",
            "data loaded\n",
            "\n",
            "model loading\n",
            "No existing model, starting training from scratch...\n",
            "model loaded\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqW5a3uaBZRA",
        "colab_type": "code",
        "outputId": "79d3578f-f9cb-443b-cca4-8e49151562b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "'''training'''\n",
        "print('start training')\n",
        "for epoch in range(start_epoch, train_epoch):\n",
        "  scheduler.step()\n",
        "  for batch_id, data in tqdm(enumerate(train_data_loader, 0), total=len(train_data_loader), smoothing=0.9):\n",
        "    points, target = data\n",
        "    points = points.data.numpy()\n",
        "    points = random_point_dropout(points)\n",
        "    points[:,:, 0:3] = random_scale_point_cloud(points[:,:, 0:3])\n",
        "    points[:,:, 0:3] = shift_point_cloud(points[:,:, 0:3])\n",
        "    points = torch.Tensor(points)\n",
        "    target = target[:, 0]\n",
        "\n",
        "    points = points.transpose(2, 1)\n",
        "    points, target = points.cuda(), target.cuda()\n",
        "    optimizer.zero_grad()    \n",
        "\n",
        "    classifier = classifier.train()\n",
        "    pred, trans_feat = classifier(points)\n",
        "    loss = criterion(pred, target.long(), trans_feat)\n",
        "    pred_choice = pred.data.max(1)[1]\n",
        "    correct = pred_choice.eq(target.long().data).cpu().sum()\n",
        "    mean_correct.append(correct.item() / float(points.size()[0]))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  \n",
        "  train_instance_acc = np.mean(mean_correct)\n",
        "  print('Train %d Accuracy: %f' % (epoch + 1,train_instance_acc))\n",
        "\n",
        "  with torch.no_grad():\n",
        "    global_epoch += 1\n",
        "    instance_acc, class_acc = test(classifier.eval(), test_data_loader)\n",
        "    if instance_acc >= best_instance_acc:\n",
        "      best_instance_acc = instance_acc   \n",
        "      best_epoch = epoch + 1\n",
        "\n",
        "    if class_acc >= best_class_acc:\n",
        "      best_class_acc = class_acc\n",
        "    if instance_acc >= best_instance_acc:\n",
        "      state = {\n",
        "        'epoch': best_epoch,\n",
        "        'instance_acc': instance_acc,\n",
        "        'class_acc': class_acc,\n",
        "        'model_state_dict': classifier.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict()\n",
        "        }\n",
        "      torch.save(state, model_save_path)\n",
        "    print('Best Instance Accuracy: %f, Class Accuracy: %f'% (best_instance_acc, best_class_acc))\n",
        "\n",
        "print('training fin!\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "load dataset....\n",
            "The size of train data is 9843\n",
            "The size of test data is 2468\n",
            "data loaded\n",
            "\n",
            "model loading\n",
            "Use pretrain model, current epoch: 51\n",
            "model loaded\n",
            "\n",
            "start training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 411/411 [36:21<00:00,  5.31s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train 52 Accuracy: 0.905008\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 103/103 [08:53<00:00,  5.18s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best Instance Accuracy: 0.906311, Class Accuracy: 0.870559\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 411/411 [24:58<00:00,  3.65s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train 53 Accuracy: 0.906275\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 103/103 [08:35<00:00,  5.01s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best Instance Accuracy: 0.911893, Class Accuracy: 0.879669\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 411/411 [25:04<00:00,  3.66s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train 54 Accuracy: 0.908455\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 103/103 [08:18<00:00,  4.84s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best Instance Accuracy: 0.911893, Class Accuracy: 0.879669\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 411/411 [25:22<00:00,  3.70s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train 55 Accuracy: 0.907492\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 103/103 [07:58<00:00,  4.65s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best Instance Accuracy: 0.911893, Class Accuracy: 0.879669\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 411/411 [23:48<00:00,  3.47s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train 56 Accuracy: 0.906995\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 103/103 [07:40<00:00,  4.47s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best Instance Accuracy: 0.923058, Class Accuracy: 0.883634\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 411/411 [23:44<00:00,  3.47s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train 57 Accuracy: 0.907509\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 103/103 [07:59<00:00,  4.65s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best Instance Accuracy: 0.923058, Class Accuracy: 0.884882\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 411/411 [23:11<00:00,  3.39s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train 58 Accuracy: 0.907760\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 103/103 [07:44<00:00,  4.51s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best Instance Accuracy: 0.923058, Class Accuracy: 0.884882\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 411/411 [23:59<00:00,  3.50s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train 59 Accuracy: 0.908772\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 103/103 [08:24<00:00,  4.90s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best Instance Accuracy: 0.923058, Class Accuracy: 0.889218\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 411/411 [23:41<00:00,  3.46s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train 60 Accuracy: 0.908737\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 103/103 [07:30<00:00,  4.38s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best Instance Accuracy: 0.923058, Class Accuracy: 0.889218\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 411/411 [22:52<00:00,  3.34s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train 61 Accuracy: 0.907634\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 103/103 [07:23<00:00,  4.31s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best Instance Accuracy: 0.923058, Class Accuracy: 0.889218\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 411/411 [22:23<00:00,  3.27s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train 62 Accuracy: 0.907238\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 103/103 [07:34<00:00,  4.41s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best Instance Accuracy: 0.923058, Class Accuracy: 0.889218\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 411/411 [22:32<00:00,  3.29s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train 63 Accuracy: 0.907813\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 103/103 [07:34<00:00,  4.41s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best Instance Accuracy: 0.923058, Class Accuracy: 0.889218\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 411/411 [23:02<00:00,  3.36s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train 64 Accuracy: 0.908619\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 103/103 [07:26<00:00,  4.33s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best Instance Accuracy: 0.923058, Class Accuracy: 0.889218\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 411/411 [23:39<00:00,  3.45s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train 65 Accuracy: 0.909672\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 103/103 [07:23<00:00,  4.31s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best Instance Accuracy: 0.923058, Class Accuracy: 0.889218\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 411/411 [22:45<00:00,  3.32s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train 66 Accuracy: 0.910591\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 103/103 [08:02<00:00,  4.69s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best Instance Accuracy: 0.923058, Class Accuracy: 0.889218\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 411/411 [22:59<00:00,  3.36s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train 67 Accuracy: 0.910945\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 103/103 [07:17<00:00,  4.25s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best Instance Accuracy: 0.923058, Class Accuracy: 0.889218\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 411/411 [23:24<00:00,  3.42s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train 68 Accuracy: 0.911353\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 103/103 [07:25<00:00,  4.33s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best Instance Accuracy: 0.923058, Class Accuracy: 0.889218\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 411/411 [22:24<00:00,  3.27s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train 69 Accuracy: 0.911840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 103/103 [07:19<00:00,  4.27s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best Instance Accuracy: 0.923058, Class Accuracy: 0.889218\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 411/411 [23:04<00:00,  3.37s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train 70 Accuracy: 0.912345\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 23%|██▎       | 24/103 [01:59<04:50,  3.67s/it]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbbyqtTSBazR",
        "colab_type": "code",
        "outputId": "c8677138-24db-435f-cf85-487fb2636aa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "source": [
        "'''test'''\n",
        "vote_num = 3\n",
        "with torch.no_grad():\n",
        "  instance_acc, class_acc = test(classifier.eval(), test_data_loader, vote_num = vote_num)\n",
        "  print('Test Instance Accuracy: %f, Class Accuracy: %f' % (instance_acc, class_acc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/103 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-b1fd7c023041>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvote_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0minstance_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvote_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvote_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test Instance Accuracy: %f, Class Accuracy: %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minstance_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-845ae0d386a7>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, loader, vote_num, num_class)\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0mmean_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m   \u001b[0mclass_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1089\u001b[0m             \"\"\"), fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1091\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1092\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m             \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}